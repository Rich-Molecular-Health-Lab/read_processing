---
title: "Remote Raw ONT Read Processing Workflow (16s Barcoded)"
author: "Alicia M. Rich, Ph.D."
date: "`r Sys.Date()`"
output:
  html_document:
    theme:
      bootswatch: litera
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: "show"
    fig_caption: true
    df_print: paged
params:
  sampleset: "bats"
  seqrun: "nwr1"
  
---

```{r, message = FALSE}
global             <- config::get(config = "default")

here::i_am("16s_ReadProcessing_RemotelyHCC.Rmd")
source(here::here(global$setup))

theme_set(theme_classic())
thematic_rmd()
thematic_on(accent = "#8785B2FF", fg = "black")

opts_chunk$set(message = FALSE,
               warning = FALSE,
               echo    = FALSE,
               include = TRUE,
               eval    = TRUE,
               comment = "")

```

# Before you begin

>You should use this script if you are processing raw, multiplexed sequencing files remotely through the Swan System on the Holland Computing Center Cluster. If you plan to run these scripts locally on the lab's laptop, you should switch to the script version that ends in `_Locally.Rmd`.

## If this is your first time

Start with the script `read_processing/RemoteHCC_FirstUse.Rmd` script. This will guide you through the steps to transfer your data and create virtual environments you will need to run the jobs below. Once you go through that process the first time, you can skip to the steps in this script.

## This Workflow

This is the current recommended pipeline for processing **raw reads obtained from the MinION Sequencer**. The ONT sequencers basically measure electrical signals as strands of DNA pass through each nanopore. The MinKNOW software uses an algorithm to convert each signal to its estimated sequence of A's/G's/C's/T's in real time, but those on-the-fly basecalls are quite messy. It is standard practice to take the original signal data after the run has completed and use a slower, more precise algorithm to re-basecall the data with greater precision.  

## Transfer sequencing data

### Sync Results to the Google Drive

>**You should do this first step from the sequencing laptop immediately after the sequencing run. Your data will automatically back up to the lab's google drive, at which point you can switch to any computer for moving the file into the HCC and remotely processing it.**

You could do this direct from the command line, but sometimes I just prefer doing things by icons. Go to the directory icon and enter the following path in the box:
```
/var/lib/minknow/data
```
From there, find the parent directory specific to your most recent sequencing run. Right click the directory icon, and then select `Copy_to` to copy the entire directory to the following path:
```
/Home/GitRepos/read_processing/seqRuns
```
Now you should rename the data directory for your sequencing run with the less cumbersome `seqrun` name that matches the syntax you used in the params of this document.

### Sync Results to HCC Work Directory

Now you sync use your cloned directory on the HCC to this working directory so that your updated dataset is available there.

# Read Processing

## Basecalling

>This step can be the most glitchy and time-consuming because of the more intensive memory requirements of the Dorado basecaller (see [here](https://github.com/nanoporetech/dorado?tab=readme-ov-file#platforms)). We can't use just any of the nodes on the Swan sever - we have to use a GPU node, and so we must also load a specific version of the dorado package provided by the HCC team.

Run the chunk below and then copy and paste the text that it creates to replace the text in the file `batch_scripts/basecall_multiplex_remote.sh`

```{terminal, warning = FALSE}
#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --job-name=basecall
#SBATCH --error=logs/basecall_%A_%a.err
#SBATCH --output=logs/basecall_%A_%a.out
#SBATCH --partition=gpu,guest_gpu
#SBATCH --constraint='gpu_v100|gpu_t4'
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=200GB
#SBATCH --gres=gpu:1

module purge
module load dorado-gpu/0.7

seqrun=params$seqrun

cd /work/richlab/aliciarich/read_processing

mkdir -p reads/basecalled

dorado basecaller sup \
    --models-directory "dorado_models" \
    "seqRuns/$seqrun/pod5" \
    --recursive \
    --no-trim \
    > "reads/basecalled/$seqrun.bam" && \
dorado summary "reads/basecalled/$seqrun.bam" > "logs/$seqrun_basecall_summary.tsv"

```

Once you ensure this script has transferred to the `read_processing/batch_scripts` path on your HCC directory, run the code below to submit the job.

```{terminal, warning = FALSE}
cd read_processing
sbatch batch_scripts/basecall_singleplex_remote.sh
```


## Alignment, Assembly, Annotation

Now we will combine all three of these steps by implementing [`wf-bacterial-genomes`](https://github.com/epi2me-labs/wf-bacterial-genomes) with Nextflow.

Again, run the chunk below and then copy and paste this into a script that you will run. Name this script `wfbacgen_remote.sh`.

```{terminal, warning = FALSE}
#!/bin/bash
#SBATCH --time=03:00:00
#SBATCH --job-name=wfbacgen
#SBATCH --error=/work/richlab/aliciarich/read_processing/logs/wfbacgen_%A_%a.err
#SBATCH --output=/work/richlab/aliciarich/read_processing/logs/wfbacgen_%A_%a.out
#SBATCH --partition=guest
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=350GB

module purge
module load nextflow

cd /work/richlab/aliciarich/read_processing

seqrun=params$seqrun

mkdir -p processed/$seqrun

nextflow run epi2me-labs/wf-bacterial-genomes \
	-profile singularity \
	--fastq "reads/filtered/$seqrun.fastq" \
	--isolates \
	--sample "$seqrun" \
  --out_dir "processed/$seqrun" \
  --threads 32

```

Once you ensure this script has transferred to the `read_processing/batch_scripts` path on your HCC directory, run the code below to submit the job.

```{terminal, warning = FALSE}
cd read_processing
sbatch batch_scripts/wfbacgen_remote.sh
```

# Next Steps

Once this run completes, make sure you transfer the directory `processed` to your local `read_processing` directory and push everything to the github repository. You will move onto analyses scripts that draw from files located in `read_processing/processed` to create new files in `bioinformatics_stats`.

